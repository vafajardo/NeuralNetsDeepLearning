{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptrons\n",
    "\n",
    "These NNs can be viewed as a succession of Perceptrons connected together. The added complexity of the MLP is that there can be 1 or more inner layers of neurons between the input layer and output layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "In this notebook, I consider a 3 layer NN (1 input layer, 1 hidden layer, and 1 output layer):\n",
    "\n",
    "- $L$ input nodes\n",
    "- $M$ hidden nodes\n",
    "- $N$ output nodes\n",
    "\n",
    "## Activation Function\n",
    "\n",
    "- Sigmoid Function:\n",
    "$$ f(h) = \\frac{1}{1+\\exp(-\\beta h)}$$\n",
    "\n",
    "## Error Function\n",
    "\n",
    "- Sum of squares of error between targets and output values:\n",
    "$$ E = \\frac{1}{2}\\sum_{k=1}^N (y_k - t_k)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl before we can walk -- Revisit Logical XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "targets = np.array([0,1,1,0]).reshape(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = 2\n",
    "N = 1\n",
    "M = 2 # 2 hidden nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Matrix\n",
    "\n",
    "There are two sets of weights for this NN, namely: the weights connecting the input layer to the hidden layer, and the weights connecting the hidden layer to the ouput layer. I'll use two matrices to store each set of weights and store these matrices in a list.\n",
    "\n",
    "- There are $(L+1)\\times M$ weights in the first set. That is $\\bf{W}_1$ is an $(L+1)\\times M$ matrix, where $w_{i,j}$ gives the weight of the connection between the $i$-th input and the $j$-th node of the hidden layer.\n",
    "- On the other hand, $\\bf{W}_2$ is an $(M+1)\\times N$ dimensional matrix. Here $w_{j,k}$ gives the weight for the connection between the $j$-th node in the hidden layer and the $k$-th output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [np.zeros(((L+1)*M)).reshape((L+1),M), np.zeros((M+1)*N).reshape(M+1,N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 0.,  0.]]), array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the weights\n",
    "\n",
    "To avoid early saturation of the sigmoid function, the weights should be saturated to smaller values (i.e., values that are away form $\\pm 1$). A common trick is to randomize the weights according to a UNIF($-1/\\sqrt{n_{In}}$,$1/\\sqrt{n_{In}}$), where $n_{In}$ is the number of nodes in the \"input\" layer to this set of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initializeWeights(nIn,nWeights,shape=None):\n",
    "    \"\"\"\n",
    "    Returns an np.array of randomized weights according to the UNIF(-1/sqrt(nIn),1/sqrt(nIn)).\n",
    "    \"\"\"\n",
    "    if shape:\n",
    "        assert type(shape) == tuple\n",
    "        return (-1/np.sqrt(nIn) + np.random.rand(nWeights)*(2/np.sqrt(nIn))).reshape(shape)\n",
    "    else:\n",
    "        return -1/np.sqrt(nIn) + np.random.rand(nWeights)*(2/np.sqrt(nIn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13210931,  0.40042168],\n",
       "       [-0.14416024,  0.30672263],\n",
       "       [ 0.18242353,  0.41802818]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1 = initializeWeights(nIn=L+1, nWeights=(L+1)*M, shape=(L+1,M))\n",
    "weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.39430462],\n",
       "       [-0.57286878],\n",
       "       [-0.44616897]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights2 = initializeWeights(nIn=M+1, nWeights=(M+1)*N, shape=(L+1,N))\n",
    "weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [weights1,weights2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the forward pass of the first input vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add a bias input\n",
    "inputswithbias = np.concatenate((np.ones(inputs.shape[0])[:,np.newaxis],inputs), axis=1) # add bias node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  1.],\n",
       "       [ 1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputswithbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thisInput = inputswithbias[0]\n",
    "thisInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13210931,  0.40042168],\n",
       "       [-0.14416024,  0.30672263],\n",
       "       [ 0.18242353,  0.41802818]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h1 = np.dot(thisInput,weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13210931,  0.40042168])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# activations of hidden nodes\n",
    "a = 1/(1+np.exp(-h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.53297938,  0.59878897])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden nodes to Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "awithbias = np.ones(M+1) # should only do this the first time the algorithm runs\n",
    "awithbias[1:] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.53297938,  0.59878897])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awithbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute activation of output nodse\n",
    "h2 = np.dot(awithbias,weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = 1/(1+np.exp(-h2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2755202])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass - back propogate the error between target and output node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute error at output\n",
    "deltaOut = (output - targets[0])*output*(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05499626])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update second set of weights (output layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Denote the inputs to the output nodes (i.e., the outputs of the hidden layer) as: $\\mathbf{a}= [a_0,a_1,\\ldots,a_M]$\n",
    "- And the error at the output nodes in the row vector $\\pmb{\\delta}_o = [\\delta_o(1),\\delta_o(2),\\ldots,\\delta_o(N)]$\n",
    "\n",
    "Then to update the second layer of weights, we compute the gradient matrix:\n",
    "$$\\pmb{\\delta}_o \\otimes \\bf{a}^T = \\begin{bmatrix}\n",
    "\\delta_o(1)\\cdot\\mathbf{a}^T,\\delta_o(2)\\cdot\\mathbf{a}^T,\\ldots,\\delta_o(N)\\cdot\\mathbf{a}^T\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "- Note that the above matrix is an $(M+1)\\times N$ matrix.\n",
    "- It follows that the update for the second set of weights is given by:\n",
    "$$\\mathbf{W}_2 \\gets \\mathbf{W}_2 - \\eta(\\pmb{\\delta}_o \\otimes \\bf{a}^T)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eta = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05499626],\n",
       "       [ 0.02931187],\n",
       "       [ 0.03293116]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incrementing matrix\n",
    "np.kron(awithbias.reshape(M+1,1), deltaOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights2 -= eta * np.kron(awithbias.reshape(M+1,1), deltaOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40805368],\n",
       "       [-0.58019675],\n",
       "       [-0.45440176]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update first set of weights (between input layer and hidden layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain rule, we can \"back propogate\" the error of the net to the hidden nodes.\n",
    "$$\\delta_h(\\zeta) = \\frac{\\partial{E}}{\\partial{h_{\\zeta}}} = \\sum_{k=1}^N \\frac{\\partial{E}}\n",
    "{\\partial{h_k^{out}}} \\frac{\\partial{h_k^{out}}}{\\partial{h_{\\zeta}}}$$\n",
    "\n",
    "- This derivative asserts that the back propogation of the error is done additively. It is important to realize here that each of the hidden nodes contribute to the activations of every output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# propogate the error to the hidden layers\n",
    "deltaHidden = (a*(1-a)) * np.dot(deltaOut,weights2[1:,].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00794246, -0.00600371])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaHidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Denote the input vector as: $\\mathbf{x} = [x_0,x_1,\\ldots,x_L]$\n",
    "- And the error at the hidden nodes in the row vector $\\pmb{\\delta_h} = [\\delta_h(1),\\delta_h(2),\\ldots,\\delta_h(M)]$\n",
    "\n",
    "Then to update the first layer of weights, we compute the necessary gradient matrix:\n",
    "$$\\pmb{\\delta}_h \\otimes \\mathbf{x}^T = \\begin{bmatrix}\n",
    "\\delta_h(1)\\cdot\\mathbf{x}^T,\\delta_h(2)\\cdot\\mathbf{x}^T,\\ldots,\\delta_h(M)\\cdot\\mathbf{x}^T\n",
    "\\end{bmatrix}$$.\n",
    "\n",
    "- It follows that the update for the second set of weights is given by:\n",
    "$$\\mathbf{W}_1 \\gets \\mathbf{W}_1 - \\eta(\\pmb{\\delta}_h \\otimes \\mathbf{x}^T)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00794246, -0.00600371],\n",
       "       [-0.        , -0.        ],\n",
       "       [-0.        , -0.        ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.kron(deltaHidden[:M],thisInput[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thisInput[:,np.newaxis] # converting a row vector into a column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights1 -= eta*np.kron(deltaHidden[:M],thisInput[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13409492,  0.40192261],\n",
       "       [-0.14416024,  0.30672263],\n",
       "       [ 0.18242353,  0.41802818]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An Important Note:** There are no connections between the bias nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Automating forward/backward phases  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "np.random.seed(1)\n",
    "weights1 = initializeWeights(nIn=L+1, nWeights=(L+1)*M, shape=(L+1,M))\n",
    "weights2 = initializeWeights(nIn=M+1, nWeights=(M+1)*N, shape=(L+1,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.09581474,  0.25440881],\n",
       "        [-0.5772182 , -0.22824668],\n",
       "        [-0.40789116, -0.47072684]]), array([[-0.3622755 ],\n",
       "        [-0.17833111],\n",
       "        [-0.11920265]]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weights1,weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eta = 0.25\n",
    "awithbias = np.ones(M+1) # initializing the vector to store activations of hidden nodes\n",
    "outputs = np.zeros(targets.shape[0])\n",
    "\n",
    "for t in range(5000): # of iterations\n",
    "    tun = np.random.permutation(inputs.shape[0])\n",
    "    for i in tun: # randomize the order in which inputs are fed to the NN\n",
    "        thisInput = inputswithbias[i]\n",
    "        # Forward phase\n",
    "        # ... computing activations of hidden nodes\n",
    "        hHidden = np.dot(thisInput,weights1)\n",
    "        a = 1/(1+np.exp(-hHidden))\n",
    "        awithbias[1:] = a\n",
    "        # ... computing activations of output nodes\n",
    "        hOut = np.dot(awithbias,weights2)\n",
    "        output = 1/(1+np.exp(-hOut))\n",
    "        outputs[i] = output\n",
    "\n",
    "        # Backward phase (Compute Error and Back-Propogate)\n",
    "        # ... update second layer of weights\n",
    "        deltaOut = (output - targets[i])*output*(1-output)\n",
    "        weights2 -= eta * np.kron(deltaOut,awithbias.reshape(M+1,1))\n",
    "        # ... update first layer of weights\n",
    "        deltaHidden = (a*(1-a)) * np.dot(deltaOut,weights2[1:,].T)\n",
    "        weights1 -= eta*np.kron(deltaHidden,thisInput[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.18818021,  6.06238454],\n",
       "        [-5.71886494, -4.0987768 ],\n",
       "        [-5.72736929, -4.10047476]]), array([[-3.60920341],\n",
       "        [-8.16405839],\n",
       "        [ 7.83638835]]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weights1,weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04182017,  0.95387693,  0.9539207 ,  0.05834152])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
